{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchtext\n",
    "from torchtext.data.utils import get_tokenizer\n",
    "from torch.utils.data import Dataset\n",
    "from torchtext.data.functional import numericalize_tokens_from_iterator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import docx\n",
    "\n",
    "def getText(filename):\n",
    "    doc = docx.Document(filename)\n",
    "    fullText = []\n",
    "    for para in doc.paragraphs:\n",
    "        fullText.append(para.text)\n",
    "    return '\\n'.join(fullText)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "notes = [getText(\"/Users/etashguha/Downloads/Sample Note.docx\")]\n",
    "labels = [0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "icd_codes = [\"Unspecified atrial fibrillation\",\"Acute rheumatic fever\", \"Minor rheumatic fever\"]\n",
    "en_tokenizer = get_tokenizer('spacy', language='en')\n",
    "\n",
    "vocab = {}\n",
    "vocab_index = 0\n",
    "for note in notes:\n",
    "    words = en_tokenizer(note)\n",
    "    for word in words:\n",
    "        if word not in vocab:\n",
    "            vocab[word] = vocab_index \n",
    "            vocab_index += 1\n",
    "for code in icd_codes:\n",
    "    words = en_tokenizer(code)\n",
    "    for word in words:\n",
    "        if word not in vocab:\n",
    "            vocab[word] = vocab_index \n",
    "            vocab_index += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(en_tokenizer(\"left shoulder\"))\n",
    "# id_iters = numericalize_tokens_from_iterator(vocab, [[\"left\", \"shoulder\"], [\"no\", \"effusion\"]])\n",
    "# for ids in id_iters:\n",
    "#     print([num for num in ids])\n",
    "\n",
    "class SATDataSet(Dataset):\n",
    "    def __init__(self, texts, labels):\n",
    "        super().__init__()\n",
    "        self.data = []\n",
    "        for i, text in enumerate(texts):\n",
    "            id_iters = numericalize_tokens_from_iterator(vocab, [en_tokenizer(text)])\n",
    "            for ids in id_iters:\n",
    "                self.data.append(([num for num in ids], labels[i]))\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return torch.tensor(self.data[idx][0]), torch.tensor(self.data[idx][1])\n",
    "train_dataset = SATDataSet(notes, labels)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "index_codes = []\n",
    "for code in icd_codes:\n",
    "    id_iters = numericalize_tokens_from_iterator(vocab, [en_tokenizer(code)])\n",
    "    for ids in id_iters:\n",
    "        index_codes.append([num for num in ids])\n",
    "\n",
    "def n_gram(codes, n):\n",
    "    return [n_gram_helper(code, n) for code in codes]\n",
    "\n",
    "def n_gram_helper(code, n):\n",
    "    l = []\n",
    "    for i in range(len(code) - n + 1):\n",
    "        l.append(code[i:i + n])\n",
    "    return l\n",
    "\n",
    "\n",
    "code_lengths = []\n",
    "for cod in index_codes:\n",
    "    code_lengths.append(len(cod))\n",
    "index_codes = [n_gram(index_codes, i) for i in range(2,4)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[[363, 244], [244, 245]], [[364, 365], [365, 366]], [[367, 365], [365, 366]]], [[[363, 244, 245]], [[364, 365, 366]], [[367, 365, 366]]]]\n",
      "[363, 244]\n",
      "(363, 244)\n",
      "[244, 245]\n",
      "(244, 245)\n",
      "[364, 365]\n",
      "(364, 365)\n",
      "[365, 366]\n",
      "(365, 366)\n",
      "[367, 365]\n",
      "(367, 365)\n",
      "[365, 366]\n",
      "(365, 366)\n",
      "[363, 244, 245]\n",
      "(363, 244, 245)\n",
      "[364, 365, 366]\n",
      "(364, 365, 366)\n",
      "[367, 365, 366]\n",
      "(367, 365, 366)\n",
      "[[{(244, 245), (363, 244)}, {(364, 365), (365, 366)}, {(365, 366), (367, 365)}], [{(363, 244, 245)}, {(364, 365, 366)}, {(367, 365, 366)}]]\n",
      "[{(363, 244): 1, (244, 245): 1, (364, 365): 1, (365, 366): 2, (367, 365): 1}, {(363, 244, 245): 1, (364, 365, 366): 1, (367, 365, 366): 1}]\n"
     ]
    }
   ],
   "source": [
    "def get_code_ngrams(index_codes):\n",
    "    freq_codes = []\n",
    "    for icd10 in index_codes:\n",
    "        d = set()\n",
    "        for gram in icd10:\n",
    "            \n",
    "            gram = tuple(gram)\n",
    "            d.add(gram)\n",
    "        freq_codes.append(d)\n",
    "    return freq_codes\n",
    "\n",
    "def get_frequency_of_codes(index_codes):\n",
    "    freq_codes = {}\n",
    "    for icd10 in index_codes:\n",
    "        for gram in icd10:\n",
    "            print(gram)\n",
    "            gram = tuple(gram)\n",
    "            print(gram)\n",
    "            if gram in freq_codes:\n",
    "                freq_codes[gram] += 1\n",
    "            else:\n",
    "                freq_codes[gram] = 1\n",
    "    return freq_codes\n",
    "print(index_codes)\n",
    "freq_codes = [get_frequency_of_codes(index_codes[i]) for i in range(2)]\n",
    "code_ngrams = [get_code_ngrams(index_codes[i]) for i in range(2)]\n",
    "print(code_ngrams)\n",
    "print(freq_codes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([  0,   1,   2,   3,   4,   3,   5,   6,   7,   8,   9,  10,  11,   5,\n",
      "         12,  13,  14,  15,  16,   2,   3,  17,  18,  19,  20,  21,  22,  23,\n",
      "         24,  25,  26,  27,  28,   2,   3,  29,  18,  30,  31,  32,  33,   2,\n",
      "          3,  17,  20,  21,  34,  35,   3,  36,  37,  38,  39,   2,   3,  40,\n",
      "         41,  42,  43,  44,  45,   2,   3,  46,  47,  48,  49,  25,  50,  19,\n",
      "         51,  52,  53,  54,  55,   5,  56,  57,  58,  52,  25,  59,   2,   3,\n",
      "         60,  61,  38,  62,  63,  25,  47,  51,  64,  65,  57,  58,  51,  52,\n",
      "         25,  66,  32,  65,  67,  68,  51,  69,   2,   3,  70,  47,  71,  56,\n",
      "         72,  14,  73,  74,  75,  65,  57,  58,   2,   3,  29,  76,  35,   7,\n",
      "          9,  14,  53,  54,   2,  77,  17,  76,  35,  78,  79,  80,  81,   2,\n",
      "          3,  17,  76,  35,  82,  83,  80,  84,  85,   2,   3,  17,  76,  35,\n",
      "         86,  87,  88,  80,  89,   2,  90,  91,  14,  92,  93,   3,  94,  95,\n",
      "         96,  14,  97,  98,   6,  99,  24,  25, 100, 101, 102, 103, 104, 105,\n",
      "        106,   8,   9, 107, 108,   2, 109, 110,  93, 109, 111, 112,  93, 113,\n",
      "        114, 115,  93, 116, 117, 118, 119, 117, 120,  93, 121, 122, 123, 117,\n",
      "        124, 125,  93, 126, 127,   3, 128, 129, 130,   3, 128, 131, 132, 133,\n",
      "        134, 119, 135, 133, 136, 127,   3, 128, 137, 138,   3, 128, 139, 140,\n",
      "        141, 142, 143,  93, 144,  52, 145, 123, 105, 146, 147,   2, 116, 148,\n",
      "         93, 149, 123, 150,   2, 151, 123, 152, 153,  44, 154,   2, 116, 155,\n",
      "         93, 156, 123, 105, 157,   2, 116, 158,  93, 159, 160, 161, 162, 123,\n",
      "        163, 164,  32, 165, 166, 167,   2, 116, 168,  93, 169, 123, 170, 123,\n",
      "        105, 171, 123, 172,  80, 173, 116, 174,  93, 175, 123, 165, 166, 176,\n",
      "        123, 177, 160, 178,  52,  41, 179, 180, 116, 181,  93, 182,  98, 183,\n",
      "         32, 184, 123, 185, 186, 116, 187,  93, 188,  41, 189,  23, 190, 191,\n",
      "          2, 116, 192,  93, 193, 123, 194, 123, 195, 196, 123, 105, 197, 198,\n",
      "        199, 116, 200,  93, 201, 202, 203, 204, 205, 123, 105, 206, 123, 207,\n",
      "         80, 208,   2, 116, 209,  93, 210, 211, 212, 213,  93, 214, 215, 216,\n",
      "         21, 217, 218, 119, 219, 220, 221,   2, 127,   2, 116, 222, 223,  24,\n",
      "        224, 225, 226,  52,  25,  26, 227, 228,   2, 212, 222, 223,  24, 224,\n",
      "        225, 229,  52,  25,  26, 227, 228,   2, 116, 222, 223,  24, 224, 225,\n",
      "        230,  52,  25,  26, 227, 228,   2, 212, 231, 232,  93, 116, 233, 234,\n",
      "        235, 119, 236, 127,  90, 237, 238,  93, 239, 116, 240, 241, 234, 242,\n",
      "        112,  93, 233, 243, 244, 245, 119, 246, 127, 247,  93, 248, 249,  93,\n",
      "        250, 251, 252, 253,  93, 254, 255, 256,   2, 257, 258,   2, 259, 260,\n",
      "        261,   2, 262, 263, 123, 264, 123,  32, 265, 266, 267,   2, 222, 268,\n",
      "        269,   2, 222, 270,  80, 271,   2, 272, 273,  44, 274,  32, 275,   5,\n",
      "        276, 277, 278,   2, 222, 146, 279, 280,   2, 233, 281, 282, 283,  32,\n",
      "        284, 285, 286,   2, 287, 288,   2, 289, 290,   2,  90, 291, 206,  80,\n",
      "        269,   2, 292, 293, 294, 295, 296,   2, 297, 298,  93, 299, 300,   3,\n",
      "        239, 301, 302,   3, 303, 212, 304,  93,   3, 305,  49, 306, 307, 244,\n",
      "        245,   5,  55,  58,  14, 308, 105, 309, 310, 311,   2,  90, 312, 313,\n",
      "        314,  93, 116, 234,   2,   3,  53,  54,  55,   2, 116, 315,  98,  56,\n",
      "        316, 317,  53, 318,   2,  90, 201,   2,   3, 319, 320, 321, 116,  17,\n",
      "        322, 294, 323,  44, 138,  32, 324, 325,   5, 326, 327,  14, 328, 329,\n",
      "        319, 320, 330,   2,   3, 331, 332,   2,  90,  27,   2,   3, 333, 334,\n",
      "        116, 335, 336, 337,   2,   3, 331, 332,   2, 116, 338, 339,   2,  90,\n",
      "        179,   2,   3, 333, 340, 116, 341, 336,  98, 234, 342,   2,   3, 331,\n",
      "        332,   2, 214, 343, 344, 345,  25,  47, 346, 347, 348, 349, 350, 201,\n",
      "        351,  52,  25, 352,  24, 353,   2, 116, 354, 355,  93, 356, 202,   3,\n",
      "        357,  90, 358, 294, 359, 314,  93, 360,   3, 361,   3, 349, 362, 212])\n"
     ]
    }
   ],
   "source": [
    "for text in train_dataset:\n",
    "    print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.6689286894688848e-06\n",
      "5.960462772236497e-07\n",
      "2.3841855067985307e-07\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n"
     ]
    }
   ],
   "source": [
    "class icd_pred(torch.nn.Module):\n",
    "    def __init__(self, num_words, word_dim=10, k=5, dilation=2,conv_dim=4, num_labels=128, label_dim=12):\n",
    "        super().__init__()\n",
    "        self.embed = torch.nn.Embedding(num_words, word_dim)\n",
    "        self.dconv = torch.nn.Conv1d(word_dim, conv_dim, k, dilation=2)\n",
    "        self.conv = torch.nn.Conv1d(word_dim, conv_dim, k)\n",
    "        self.u = torch.nn.Parameter(torch.rand(conv_dim, num_labels))\n",
    "        self.smax = torch.nn.Softmax(dim=2)\n",
    "        self.reduce = torch.nn.Linear(conv_dim * 2 + 2,1)\n",
    "        \n",
    "    def forward(self, sentence):\n",
    "        text_embedded = self.embed(sentence)\n",
    "        text_embedded = text_embedded.unsqueeze(dim=0).transpose(1,2)\n",
    "        #dilated conv\n",
    "        H = self.dconv(text_embedded).transpose(1,2)\n",
    "        alpha = self.smax(torch.matmul(H, self.u))\n",
    "        H = H.squeeze(dim=0).transpose(0,1)\n",
    "        alpha = alpha.squeeze(dim=0)\n",
    "        dilated_m = torch.matmul(H, alpha)\n",
    "        \n",
    "        #normal conv\n",
    "        H = self.conv(text_embedded).transpose(1,2)\n",
    "        alpha = self.smax(torch.matmul(H, self.u))\n",
    "        H = H.squeeze(dim=0).transpose(0,1)\n",
    "        alpha = alpha.squeeze(dim=0)\n",
    "        normal_m = torch.matmul(H, alpha)\n",
    "        \n",
    "        scores = [self.get_ngram_scores(sentence, i) for i in range(2,4)]\n",
    "        scores = torch.tensor(scores)\n",
    "        final_scores = torch.cat([normal_m, dilated_m, scores]).transpose(0,1)\n",
    "        \n",
    "        final_scores = self.reduce(final_scores)\n",
    "        return final_scores.squeeze()\n",
    "    def get_ngram_scores(self, sentence, n):\n",
    "        gram_freq_text = self.getfreqgrams(sentence, n)\n",
    "        listofscores =[]\n",
    "        for i, code in enumerate(code_ngrams[n - 2]):\n",
    "            score = 0\n",
    "            for gram in code:\n",
    "                if gram in gram_freq_text:\n",
    "                    score += gram_freq_text[gram] * n/code_lengths[i] * len(code_ngrams[n - 2 ])/freq_codes[n - 2][gram]\n",
    "            listofscores.append(score)\n",
    "        return listofscores\n",
    "            \n",
    "    def getfreqgrams(self, sentence, n):\n",
    "        sentence = sentence.tolist()\n",
    "        freq_gram = {}\n",
    "        n_grams = n_gram_helper(sentence, n)\n",
    "        \n",
    "        for n_gram in n_grams:\n",
    "            n_gram = tuple(n_gram)\n",
    "            if n_gram in freq_gram:\n",
    "                freq_gram[n_gram] += 1\n",
    "            else:\n",
    "                freq_gram[n_gram] = 1\n",
    "        return freq_gram\n",
    "    \n",
    "    def n_gram_helper(self, code, n):\n",
    "        l = []\n",
    "        for i in range(len(code) - n + 1):\n",
    "            l.append(code[i:i + n])\n",
    "        return l\n",
    "        \n",
    "model = icd_pred(len(vocab), num_labels=3)\n",
    "loss_fn = torch.nn.CrossEntropyLoss()\n",
    "optim = torch.optim.Adam(model.parameters())\n",
    "for _ in range(100):\n",
    "    for text, label in train_dataset:\n",
    "        optim.zero_grad()\n",
    "        answers = model(text)\n",
    "        answers = answers.unsqueeze(dim=0)\n",
    "        label = label.unsqueeze(dim=0)\n",
    "\n",
    "        loss = loss_fn(answers, label)\n",
    "        loss.backward()\n",
    "        print(loss.item())\n",
    "        optim.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
